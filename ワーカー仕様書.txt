インポート後などに頻回して行うバッチ処理
worker_jobsのpriority(昇順) > created_at(昇順)の順番で、job_typeによって異なる種類の処理を行っていく。
別プロジェクトでOllamaを用いて処理を行うコードがあるので、参考に記載します。

OLLAMA_URL = "http://localhost:11434/api/generate"

def call_ollama(prompt: str) -> dict[str, Any]:
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0,
            "top_p": 0.8,
            "repeat_penalty": 1.1,
        },
    }
    req = urllib.request.Request(
        OLLAMA_URL,
        data=json.dumps(payload).encode("utf-8"),
        headers={"Content-Type": "application/json"},
        method="POST",
    )
    logger.info("Ollama request prompt: %s", prompt)
    with urllib.request.urlopen(req, timeout=300000) as resp:
        body = resp.read().decode("utf-8")
        logger.info("Ollama response status=%s body=%s", resp.status, body)
        return json.loads(body)


【utterance_role】
・utterance_roles一覧を取得し、uttelance_role_nameを「/」区切りで連結する = allowed_terms
・MODEL_NAME = "gpt-oss:20b"
・prompt ="あなたは分類器です。出力は2行のみ。\n"
"1行目：許可単語一覧から1つを完全一致で出力。\n"
"2行目：自信度を0.00〜1.00で出力。\n"
"他の文章は禁止。\n\n"
"許可単語一覧：\n"
f"{allowed_terms}\n\n"
"contents：\n"
f"{utterance['contents']}"
・response受け取り後
　　・response内で一番最初に出てきたutterance_role_nameに完全一致するutterance_role_idを、utterance.utterance_role_idに格納
　　・response内の数値のうち、最小値をutterance.utterance_role_confidenceに格納。

【did_asked_knowledge】
・MODEL_NAME = "gpt-oss:20b"
・prompt ="以下のテキストから、次のいずれかの形で明示されている内容だけを抽出し、区切り線「---」を設けて列挙してください。\n"
"- 定義（AとはBである）\n"
"- 因果関係（AするとBになる）\n"
"- 一般的な規則・指針（〜すべき／〜するとよい）\n"
"- 手順・方法（〜するには、まず〜） \n\n"
"他の文章は禁止。該当するものがない場合は何も返さないでください。\n"
"日本語で返却してください。\n\n"
"contents：\n"
f"{utterance['contents']}"
・response受け取り後
　　・response内を「---」で区切り、それらをseedsにINSERTする
　　　　・seed_id = 生成UUID
　　　　・seed_type = seed
　　　　・body = 区切ったテキスト
　　　　・created_from = utterance
　　　　・review_status = auto
　　　　・created_at = 現在時刻
　　　　・updated_at = 現在時刻
　　・utterance_seedsにINSERTする
　　　　・utterance_id = 読み込ませたutteranceのutterance_id
　　　　・seed_id = 生成したseed_id
　　　　・relation_type = derived_from
　　　　・created_at = 現在時刻
　　・worker_jobsにINSERTする
　　　　・job_id = 生成UUID
　　　　・job_type = embedding
　　　　・target_table = seed
　　　　・target_id = 生成seed_id
　　　　・status = queued
　　　　・priority = 10
　　　　・created_at = 現在日時
　　　　・updated_at = 現在日時
　　・utterancesをUPDATEする
　　　　・did_asked_knowledge = 1
　　　　・updated_at = 現在日時

【embedding】
・MODEL_NAME = "paraphrase-multilingual:latest"
・prompt = f"{worker_jobs.target_table['contents']}"
・response受け取り後
　　・responseの内容をL2正規化する
　　・embeddingsにINSERTする
　　　　・embedding_id = 生成UUID
　　　　・target_type = worker_jobs.target_table
　　　　・target_id = worker_jobs.target_id
　　　　・model_name = paraphrase-multilingual
　　　　・dims = ベクトルの次元数（要素数）
　　　　・vector = L2正規化したresponse(正規化に失敗した場合はresponseの内容)
　　　　・is_l2_normalized = 1(正規化に失敗した場合は0)
　　　　・created_at = 現在時刻
　　（以降はseed・clusterのみ実行）
　　・embeddings内でvectorを用いて近傍を検索し、上位20個のseed・clusterのうち、clusterが存在した場合には上位2個に対してedgesをINSERTする。
　　　　・edge_id = 生成UUID
　　　　・src_type = worker_jobs.target_table
　　　　・src_id = worker_jobs.target_id
　　　　・dst_type = cluster
　　　　・dst_id = 該当cluster_id
　　　　・edge_type = part_of
　　　　・weight = 該当clusterとworker_jobsで指定した要素の類似度
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　　　・updated_at = 現在時刻
　　・clusterにedgesを追加した場合、layout_pointsをINSERTする。
　　　　・layout_id = layout_ruins.scope_cluster_id = 該当cluster_idに紐づいているlayout_id
　　　　・target_type = worker_jobs.target_table
　　　　・target_id = worker_jobs.target_id
　　　　・x = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のxの平均値
　　　　・y = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のyの平均値
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　・clusterにedgesを追加したtarget_typeがseedの場合、utterance_seedで紐づいているutteranceすべてに対して、layout_pointsをINSERTする。
　　　　・layout_id = layout_ruins.scope_cluster_id = 該当cluster_idに紐づいているlayout_id
　　　　・target_type = utterance
　　　　・target_id = 該当のutterance_id
　　　　・x = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のxの平均値
　　　　・y = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のyの平均値
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　・clusterにedgesを追加したtarget_typeがseedの場合、utterance_seedで紐づいているutteranceすべてに対して、layout_runs.scope_type = globalに紐づいているlayout_printsが存在したら、layout_points.is_active = 0にUPDATEする。
　　・clusterにedgesを追加しなかった場合、layout_runs.scope_type = globalのlayout_runsに紐づくlayout_pointsをINSERTする。
　　　　・layout_runs.scope_type = globalのレコードがない場合、以下をINSERTする
　　　　　　・layout_id = 生成UUID
　　　　　　・algorithm = umap
　　　　　　・dims = 2
　　　　　　・scope_type = global
　　　　　　・params_json = {"n_components":2, "n_neighbors":15, "min_dist":0.1, "random_state":42}
　　　　　　・is_active = 1
　　　　　　・created_at = 現在時刻
　　　　・layout_pointsをINSERTする
　　　　　　・layout_id = layout_runs.scope_type = globalのlayout_runsのlayout_id
　　　　　　・target_type = worker_jobs.target_table
　　　　　　・target_id = worker_jobs.target_id
　　　　　　・x = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のxの平均値
　　　　　　・y = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のyの平均値
　　　　　　・is_active = 1
　　　　　　・created_at = 現在時刻
　　・clusterにedgesを追加せずtarget_typeがseedの場合、utterance_seedで紐づいているutteranceすべてに対して、layout_pointsをINSERTする。
　　　　・layout_id = layout_runs.scope_type = globalのlayout_runsのlayout_id
　　　　・target_type = utterance
　　　　・target_id = 該当のutterance_id
　　　　・x = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のxの平均値
　　　　・y = layout_runsに紐づいているlayout_pointsのうち、近傍検索で上位10件のyの平均値
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　・embeddings内でvectorを用いて近傍を検索し、上位10個のseed・clusterに対してedgesをINSERTする。
　　　　・edge_id = 生成UUID
　　　　・src_type = worker_jobs.target_table
　　　　・src_id = worker_jobs.target_id
　　　　・dst_type = 該当要素名(utterance / seed / cluster)
　　　　・dst_id = 該当要素id(utterance_id / seed_id / cluster_id)
　　　　・edge_type = near
　　　　・weight = 該当clusterとworker_jobsで指定した要素の類似度
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　　　・updated_at = 現在時刻

【embedding_utterance】
・utterance_idに紐づいているutterance_splitsを列挙する
・各utterance_splitsに紐づいているembeddingsを列挙する
・全てのutterance_splitsにembeddingsが紐づいている、かつ全てのembeddingsがis_l2_normalized = 1の場合、該当するすべてのvectorの平均を取り、L2正規化を行う。
・embeddingsにINSERTする
　　・embedding_id = 生成UUID
　　・target_type = utterance
　　・target_id = work_jobs.target_id
　　・model_name = 任意のutterance_splitsに紐づいているembeddings.model_name
　　・dims = 任意のutterance_splitsに紐づいているembeddings.model_name
　　・vector = 平均化＆L2正規化した値
　　・is_l2_normalized = 1
　　・created_at = 現在時刻

【cluster_body】
・edgesのうち、dst_type = cluster、dst_id = worker_jobs.target_id、edge_type = part_ofに該当するseedのcontensを改行区切りで連結する。
・MODEL_NAME = "gpt-oss:20b"
・prompt ="以下のテキストから概要・まとめを出力してください。\n"
"他の文章は禁止。\n\n"
"contents：\n"
f"{連結したテキスト}"
・response受け取り後
　　・clusters.cluster_id = worker_jobs.target_idにUPDATEする
　　　　・cluster_overview = 受け取ったresponse
　　　　・updated_at = 現在時刻

---

夜間などに行う重たいバッチ処理
・全seedの近傍距離の平均をall_seed_info.avg_seed_distanceとして、近傍距離の中央値をall_seed_info.median_seed_distanceとしてINSERTorUPDATEする。

・clustersテーブル・layout_runsテーブル・layout_pointsテーブルの全データをDELETEする

・clusterの生成
以下の仕様で「seed・clusterに紐づくembeddings.vectorから、局所密度が異常に高い部分をclusterとして抽出し、さらにcluster内で再帰的に同様の抽出を行う」処理を実装してください。

# 目的
- seed・clusterの embedding ベクトル群から「局所的にめちゃくちゃ密度が高い塊」を cluster として切り出す。
- cluster 内にさらに密な塊があれば、再帰的に sub-cluster を作る（階層構造）。
- cluster が爆増しないように、明確な停止条件とゲートを持たせる。

# 入力
- seeds: list[Item]
  - Item:
    - id: str
    - vec: np.ndarray shape=(D,) float32 or float64  (embedding)
- distance metric:
  - 原則 cosine distance を使う（1 - cosine similarity）
  - ただし、入力 vec が L2正規化済みなら cosine distance は 1 - dot(u,v) で計算して良い
- params: ClusterParams
  - k_min: int = 5
  - k_max: int = 30
  - k_scale: float = 3.0   # k = clamp(round(log(n)*k_scale), k_min, k_max)
  - percentile: float = 1.0 # d_k の下位 percentile% を候補点にする
  - min_cluster_size: int = 8
  - max_depth: int = 8
  - improvement_alpha: float = 0.75
    - 再帰ゲート: 子clusterの median(d_k) <= improvement_alpha * 親集合の median(d_k) を満たす場合のみ、子cluster内をさらに再帰探索する
  - max_clusters_total: int = 5000  # 安全装置（上限を超えたら例外 or 以降の生成停止）

# 出力
- 生成されたcluster毎に生成
  - clustersテーブルにINSERT
    - cluster_id = 生成UUID
    - cluster_overview = 近傍上位5件のseedのseeds.bodyを改行区切りで連結
    - cluster_leve = 'cluster'
    - is_archived = '0'
    - created_at = 現在時刻
    - updated_at = 現在時刻
　- layout_runsテーブルにINSERT
    - layout_id = 生成UUID
    - algorithm = 'umap'
    - dims = '2'
    - scope_type = 'cluster'
    - scope_cluster_id = 上記のclusters.cluster_id
    - params_json = {"n_components": 2, "n_neighbors": 15, "min_dist": 0.1, "random_state": 42}
    - is_active = '1'
    - created_at = 現在時刻
  - clusterに所属するseed・cluster情報、clusterに所属するseedに紐づくutterance情報をlayout_pointsにISERT　※下位のcluster内の情報は含まない
    - layout_id = 生成UUID
    - target_type = 'cluster','seed','utterance'
    - target_id = 'cluster_id','seed_id','utterance_id'
    - x = '0'
    - y = '0'
    - is_active = '1'
    - created_at = 現在時刻

- clusterに所属しなかった情報
　- layout_runsテーブルにINSERT
    - layout_id = 生成UUID
    - algorithm = 'umap'
    - dims = '2'
    - scope_type = 'global'
    - params_json = {"n_components": 2, "n_neighbors": 15, "min_dist": 0.1, "random_state": 42}
    - is_active = '1'
    - created_at = 現在時刻
  - clusterに所属しなかったseed・cluster情報、clusterに所属しなかったseedに紐づくutterance情報をlayout_pointsにINSERT　※下位のcluster内の情報は含まない
    - layout_id = 生成UUID
    - target_type = 'cluster','seed','utterance'
    - target_id = 'cluster_id','seed_id','utterance_id'
    - x = '0'
    - y = '0'
    - is_active = '1'
    - created_at = 現在時刻

# アルゴリズム仕様（必須）
## 1) kNN密度スコア
- S を対象集合（itemsの部分集合）とする。
- n = |S|
- k = clamp(round(log(n) * k_scale), k_min, min(k_max, n-1))
  - n <= min_cluster_size の場合は、その集合では cluster を生成しない（即停止）
- 各点 i について、他点との距離を計算し、k番目に近い距離 d_k(i) を求める。
- 密度は d_k が小さいほど高いとみなす。（ρ = 1/(d_k+eps) を明示的に出しても良いが、判定は d_k で行う）

## 2) 候補点抽出（異常に密な点）
- d_k(i) の分布の下位 percentile% を閾値にして候補点 C を作る:
  - threshold = percentile(dk_values, percentile)
  - C = { i | d_k(i) <= threshold }
- C が min_cluster_size 未満なら cluster 生成しない（停止）

## 3) 候補点の「塊」切り出し = 近傍グラフの連結成分
- C 上で kNN グラフを作る（無向で良い）
  - 各点は、C 内の近い順に min(k, |C|-1) 個へエッジ
- グラフの連結成分ごとに component を得る。
- component サイズ < min_cluster_size は捨てる。

## 4) cluster生成と再帰
- component ごとに Cluster を生成する（member_ids = componentの点 id）
- 各 cluster について、再帰探索するかを決める:
  - 親集合の median(d_k) を M_parent とする（親集合=いま処理している S）
  - 子cluster集合（component）内で、同様に d_k を計算し median を M_child とする
  - M_child <= improvement_alpha * M_parent を満たす場合のみ、その子集合で再帰を継続
- max_depth を超えたら停止
- max_clusters_total を超えたら停止（例外 or 生成停止。どちらにするか実装方針をコメントで明示）

# 実装要件
- 距離計算はスケールに配慮すること:
  - n が大きい場合を考慮し、可能なら sklearn.neighbors.NearestNeighbors（metric='cosine'）を利用して良い。

# エッジケース
- n が小さい（n<=k_min, n<=min_cluster_size）
- C が空、あるいは連結成分が全部 min_cluster_size 未満
- 同一点（距離0）が大量にある（d_k=0になる）場合のeps対策
- 全部が等間隔っぽい場合（候補点が少ない/多い）でも暴走しないこと


edgesの再生成
・各seedに対して以下の処理を行う。
　　・embeddings内でvectorを用いて近傍を検索し、上位20個のutterance・seed・clusterのうち、clusterが存在した場合には上位2個に対してedgesをINSERTする。既に存在する場合には何もしない。不要となったedgesが存在する場合には、is_active = 0、updated_at = 現在時刻で更新する。
　　　　・edge_id = 生成UUID
　　　　・src_type = worker_jobs.target_table
　　　　・src_id = worker_jobs.target_id
　　　　・dst_type = cluster
　　　　・dst_id = 該当cluster_id
　　　　・edge_type = part_of
　　　　・weight = 該当clusterとworker_jobsで指定した要素の類似度
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　　　・updated_at = 現在時刻
　　・embeddings内でvectorを用いて近傍を検索し、上位10個のutterance・seed・clusterに対してedgesをINSERTする。既に存在する場合には何もしない。不要となったedgesが存在する場合には、is_active = 0、updated_at = 現在時刻で更新する。
　　　　・edge_id = 生成UUID
　　　　・src_type = worker_jobs.target_table
　　　　・src_id = worker_jobs.target_id
　　　　・dst_type = 該当要素名(utterance / seed / cluster)
　　　　・dst_id = 該当要素id(utterance_id / seed_id / cluster_id)
　　　　・edge_type = near
　　　　・weight = 該当clusterとworker_jobsで指定した要素の類似度
　　　　・is_active = 1
　　　　・created_at = 現在時刻
　　　　・updated_at = 現在時刻

layoutsの再計算
・UMAPを全てのlayout_runsそれぞれに対して実施する。layout_runsに紐づいているlayout_printsの要素のembeddings.venderを用いてx・yを算出する。

